{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jtt8ExWqXwqI",
        "outputId": "b241b18a-575d-485d-b66d-a5099e9b0c49"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting ijson\n",
            "  Downloading ijson-3.2.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (111 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/111.8 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.7/111.8 kB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m111.8/111.8 kB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: ijson\n",
            "Successfully installed ijson-3.2.3\n"
          ]
        }
      ],
      "source": [
        "pip install ijson"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install rank_bm25"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kjIt4oWocvpQ",
        "outputId": "28f85112-f82f-46d3-86cd-9cba29bcff9f"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting rank_bm25\n",
            "  Downloading rank_bm25-0.2.2-py3-none-any.whl (8.6 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from rank_bm25) (1.23.5)\n",
            "Installing collected packages: rank_bm25\n",
            "Successfully installed rank_bm25-0.2.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install hnswlib\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KNI9F70oX9gE",
        "outputId": "6bb06e41-7e81-4bee-9f7a-4a1745f4ce73"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting hnswlib\n",
            "  Using cached hnswlib-0.8.0.tar.gz (36 kB)\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from hnswlib) (1.23.5)\n",
            "Building wheels for collected packages: hnswlib\n",
            "  Building wheel for hnswlib (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for hnswlib: filename=hnswlib-0.8.0-cp310-cp310-linux_x86_64.whl size=2287619 sha256=bcec5c47b0448beee8ac776e612f74c272aa1a55f3016506cb11096a0d56aae5\n",
            "  Stored in directory: /root/.cache/pip/wheels/af/a9/3e/3e5d59ee41664eb31a4e6de67d1846f86d16d93c45f277c4e7\n",
            "Successfully built hnswlib\n",
            "Installing collected packages: hnswlib\n",
            "Successfully installed hnswlib-0.8.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import ijson\n",
        "import numpy as np\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "#from krovetzstemmer import Stemmer\n",
        "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n",
        "from transformers import BertTokenizer, BertModel\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "import time\n",
        "import pandas as pd\n",
        "import ast\n",
        "import hnswlib\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "skB7uy7kX1-Z"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from rank_bm25 import BM25Okapi"
      ],
      "metadata": {
        "id": "X-32Ns9zdxON"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_top_k_aip(query,docs,categories,scores,k):\n",
        "    res = sorted(list(zip(docs,categories,scores)),key = lambda x:x[2],reverse=True)\n",
        "    aip = \"\"\n",
        "    for i in range(min(k,len(scores))):\n",
        "        ppep = 'the category for the article:\"'+res[i][0]+'\" is \"'+res[i][1]+'\", and '\n",
        "        aip +=ppep\n",
        "    aip = aip[:-6] + \".\"+query\n",
        "    return aip\n",
        "\n",
        "def preprocess(text,stopwords,stemmer):\n",
        "    text = text.split()\n",
        "    text = [text[i] for i in range(len(text)) if text[i] not in stopwords]\n",
        "    text = [stemmer.stem(text[i]) for i in range(len(text))]\n",
        "    return text\n",
        "def get_data(file_path):\n",
        "    all_data = []\n",
        "    with open(file_path, 'r') as file:\n",
        "        json_objects = ijson.items(file, 'item')\n",
        "        for data in json_objects:\n",
        "            all_data.append(data)\n",
        "    f1 = open('all_data.txt','w+')\n",
        "    f1.write(str(all_data))\n",
        "    f1.close()\n",
        "    articles = []\n",
        "    for i,data in enumerate(all_data):\n",
        "    ###Uncomment if using train data\n",
        "        if i!=2112:\n",
        "            articles.append(data['input'].split('article:')[-1])\n",
        "        else:\n",
        "            articles.append(data['input'].split('article:')[-2]+\" \"+data['input'].split('article:')[-1])\n",
        "    return all_data,articles\n",
        "\n",
        "def format_aips(aipfile,outputsfile):\n",
        "    f = open(aipfile,\"r\")\n",
        "    print(aipfile)\n",
        "    aip = eval(f.read())\n",
        "    aip = aip[:101]\n",
        "    f2 = open(outputsfile)\n",
        "    y_true = json.load(f2)\n",
        "    y_true = y_true[\"golds\"]\n",
        "    y_true = [i['output'] for i in y_true]\n",
        "\n",
        "    llm_data = pd.DataFrame({\n",
        "        'text':aip,\n",
        "        'label':y_true\n",
        "    })\n",
        "    op_name = str(aipfile).split('.txt')[0]+\"_llm_output_100.csv\"\n",
        "    llm_data.to_csv(op_name)\n",
        "    return"
      ],
      "metadata": {
        "id": "-4BEdGaFYPAI"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def hnsw(all_data,vectorizer):\n",
        "    aip = []\n",
        "    all_scores = []\n",
        "    count=0\n",
        "    for instance in all_data:\n",
        "        query = instance['input']\n",
        "        docs = [instance['profile'][i]['text'] for i in range(len(instance['profile']))]\n",
        "        titles = [instance['profile'][i]['title'] for i in range(len(instance['profile']))]\n",
        "        document_vectors = vectorizer.fit_transform(docs).toarray()\n",
        "        dim = document_vectors.shape[1]\n",
        "        num_elements = len(docs)\n",
        "        index = hnswlib.Index(space='cosine', dim=dim)\n",
        "        index.init_index(max_elements=num_elements, ef_construction=50, M=20)\n",
        "        index.add_items(document_vectors)\n",
        "        #decide how much ef\n",
        "        index.set_ef(50)\n",
        "        query_vector = vectorizer.transform([query]).toarray()[0]\n",
        "        max_score_index, distances = index.knn_query(query_vector, k=num_elements)\n",
        "        max_score_index = max_score_index.tolist()\n",
        "        distances = distances.tolist()\n",
        "        res = sorted(list(zip(max_score_index[0],distances[0])))\n",
        "        scores = [1-res[i][1] for i in range(len(res))]\n",
        "        all_scores.append(scores)\n",
        "        title = titles[max_score_index[0][0]]\n",
        "        op = '\"'+title+'\" is the title for \"'+ docs[max_score_index[0][0]] +'\".'+query\n",
        "        aip.append(op)\n",
        "        count+=1\n",
        "        print(count)\n",
        "    return aip,all_scores\n"
      ],
      "metadata": {
        "id": "9IKW8wMTYR_j"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def ensemble(all_data,models):\n",
        "    n = len(all_data)\n",
        "    aip = []\n",
        "    txt = \"the category for the article: \"\n",
        "    num_models = len(models)\n",
        "    all_overall = []\n",
        "    for i in range(n):\n",
        "        query = all_data[i]['input']\n",
        "        categories = [all_data[i]['profile'][j]['category'] for j in range(len(all_data[i]['profile']))]\n",
        "        docs  = [all_data[i]['profile'][j]['text'] for j in range(len(all_data[i]['profile']))]\n",
        "        ndocs = len(docs)\n",
        "        overall_scores = [0 for i in range(ndocs)]\n",
        "        for j in range(num_models):\n",
        "            overall_scores = [overall_scores[k]+models[j][i][k] for k in range(ndocs)]\n",
        "        overall_scores = [(1/num_models)*overall_scores[k] for k in range(ndocs)]\n",
        "        aip.append(news.get_top_k_aip(query,docs,categories,overall_scores,5))\n",
        "        all_overall.append(overall_scores)\n",
        "        #max_score_index = overall_scores.index(max(overall_scores))\n",
        "        #op = '\"'+txt +'\"'+docs[max_score_index]+'\" is \"'+categories[max_score_index]+'\"\".'+all_data[i]['input']\n",
        "        #aip.append(op)\n",
        "    return aip, all_overall"
      ],
      "metadata": {
        "id": "PQdn9TVAYnFl"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def bm25_model(all_data,articles):\n",
        "    aip=[]\n",
        "    all_scores = []\n",
        "    #else:\n",
        "        #articles.append(data['input'].split('article:')[-2]+\" \"+data['input'].split('article:')[-1])\n",
        "    #print(len(articles),len(all_data))\n",
        "    txt = \"the category for the article: \"\n",
        "    for i in range(len(all_data)):\n",
        "        query = articles[i]\n",
        "        #query = all_data[i]['input']\n",
        "        docs = [all_data[i]['profile'][j]['text'] for j in range(len(all_data[i]['profile'])) ]\n",
        "        tokenized_docs = [doc.split() for doc in docs]\n",
        "        #print(\"docs\",docs)\n",
        "        category = [all_data[i]['profile'][j]['category'] for j in range(len(all_data[i]['profile'])) ]\n",
        "        bm25 = BM25Okapi(tokenized_docs)\n",
        "        scores = bm25.get_scores(query.split())\n",
        "        #min max normalization\n",
        "        min_score = min(scores)\n",
        "        max_score = max(scores)\n",
        "        if max_score == min_score:\n",
        "            scores = [1.0 for i in range(len(scores))]\n",
        "        else:\n",
        "            scores = [(score- min_score)/(max_score-min_score)  for score in scores]\n",
        "        all_scores.append(scores)\n",
        "        max_score_index = np.argmax(scores)\n",
        "        op = '\"'+txt +'\"'+docs[max_score_index]+'\" is \"'+category[max_score_index]+'\"\".'+all_data[i]['input']\n",
        "        aip.append(op)\n",
        "    #print(\"IN BM25\",len(aip))\n",
        "    return aip,all_scores\n"
      ],
      "metadata": {
        "id": "trc5SW3TZKt_"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def bert(all_data,tokenizer,model):\n",
        "    aip = []\n",
        "    all_scores=[]\n",
        "    progress_bar = tqdm(range(len(all_data)))\n",
        "    txt = \"the category for the article: \"\n",
        "    for instance in all_data:\n",
        "        query = instance['input']\n",
        "        docs = [instance['profile'][i]['text'] for i in range(len(instance['profile']))]\n",
        "        category =[instance['profile'][i]['category'] for i in range(len(instance['profile']))]\n",
        "        document_tokens = tokenizer.batch_encode_plus(docs, padding=True, truncation=True, return_tensors='pt').to('cuda')\n",
        "        query_tokens = tokenizer.encode_plus(query, padding=True, truncation=True, return_tensors='pt').to('cuda')\n",
        "        document_embeddings = model(**document_tokens).last_hidden_state.mean(dim=1)  # Average pooling over tokens\n",
        "        query_embedding = model(**query_tokens).last_hidden_state.mean(dim=1).squeeze()\n",
        "        similarities = cosine_similarity(query_embedding.reshape(1, -1).detach().cpu().numpy(), document_embeddings.detach().cpu().numpy()).squeeze()\n",
        "        progress_bar.update(1)\n",
        "        ranking = sorted(enumerate(similarities), key=lambda x: x[1], reverse=True)\n",
        "        #print(ranking)\n",
        "        max_score_index = ranking[0][0]\n",
        "        op = '\"'+txt +'\"'+docs[max_score_index]+'\" is \"'+category[max_score_index]+'\"\".'+instance['input']\n",
        "        aip.append(op)\n",
        "        ranking = sorted(ranking)\n",
        "        scores = [ranking[i][1] for i in range(len(ranking))]\n",
        "        #print(\"SCORES\",ranking,scores)\n",
        "        all_scores.append(scores)\n",
        "    return aip, all_scores"
      ],
      "metadata": {
        "id": "tWQvkdH3arAN"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def fwrite(output,filename):\n",
        "    f = open(filename,'w+')\n",
        "    f.write(str(output))\n",
        "    f.close()\n",
        "    return"
      ],
      "metadata": {
        "id": "-s1NoQDJZx4U"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "file_path = 'train_questions.json'\n",
        "op_file_path = 'train_outputs.json'\n",
        "all_data, articles = get_data(file_path)"
      ],
      "metadata": {
        "id": "KFJs8kYXZYzP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bm25_output,bm25_scores = bm25.bm25_model(all_data,articles)\n",
        "fwrite(bm25_output,'bm_25_aip.txt')\n",
        "fwrite(bm25_scores,'bm_25_scores.txt')"
      ],
      "metadata": {
        "id": "fH8amQydZajD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "model = BertModel.from_pretrained('bert-base-uncased')\n",
        "bert_output,bert_scores = bert.bert(all_data,tokenizer,model)\n",
        "model.to('cuda')\n",
        "fwrite(bert_output,'bert/train/bert_aip_train.txt')\n",
        "fwrite(bert_scores,'bert/train/bert_scores_train.txt')"
      ],
      "metadata": {
        "id": "i3Pc_ztHZrST"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}